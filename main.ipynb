{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "MotsVides = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Extraction automatique de termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'D.Z.', 'poster-print', 'costs', '120.50DA...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Texte = \"That D.Z. poster-print costs 120.50DA...\"\n",
    "Termes = Texte.split()\n",
    "Termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'D', 'Z', 'poster', 'print', 'costs', '120', '50DA']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpReg = nltk.RegexpTokenizer('\\w+')\n",
    "Termes = ExpReg.tokenize(Texte)\n",
    "Termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'D', 'Z', 'poster', 'print', 'costs', '120', '50DA']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpReg = nltk.RegexpTokenizer('\\w+|(?:[A-Z]\\.)+')\n",
    "Termes = ExpReg.tokenize(Texte)\n",
    "Termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'D.Z.', 'poster', 'print', 'costs', '120', '50DA']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpReg = nltk.RegexpTokenizer('(?:[A-Z]\\.)+|\\w+')\n",
    "Termes = ExpReg.tokenize(Texte)\n",
    "Termes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'D.Z.', 'poster', 'print', 'costs', '120', '50DA', '...']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpReg = nltk.RegexpTokenizer('(?:[A-Z]\\.)+|\\w+|\\.{3}')\n",
    "Termes = ExpReg.tokenize(Texte)\n",
    "Termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'D.Z.', 'poster', 'print', 'costs', '120.50DA', '...']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpReg = nltk.RegexpTokenizer('(?:[A-Z]\\.)+|\\d+(?:\\.\\d+)?DA?|\\w+|\\.{3}')\n",
    "Termes = ExpReg.tokenize(Texte)\n",
    "Termes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Suppression des mots-vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'D.Z.', 'poster', 'print', 'costs', '120.50DA', '...']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpReg = nltk.RegexpTokenizer('(?:[A-Z]\\.)+|\\d+(?:\\.\\d+)?DA?|\\w+|\\.{3}')\n",
    "Termes = ExpReg.tokenize(Texte)\n",
    "Termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "MotsVides = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Termes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m TermesSansMotsVides \u001b[39m=\u001b[39m [terme \u001b[39mfor\u001b[39;00m terme \u001b[39min\u001b[39;00m Termes \u001b[39mif\u001b[39;00m terme\u001b[39m.\u001b[39mlower() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m MotsVides]\n\u001b[1;32m      2\u001b[0m TermesSansMotsVides\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Termes' is not defined"
     ]
    }
   ],
   "source": [
    "TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "TermesSansMotsVides"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 4 - Normalisation (stemming) des termes extraits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d.z.', 'poster', 'print', 'cost', '120.50da', '...']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Porter = nltk.PorterStemmer()\n",
    "TermesNormalisation = [Porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "TermesNormalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d.z.', 'post', 'print', 'cost', '120.50da', '...']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lancaster = nltk.LancasterStemmer()\n",
    "TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "TermesNormalisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CrÃ©ation d'index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TermesNormalisation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m TermesFrequence \u001b[39m=\u001b[39m {}\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m terme \u001b[39min\u001b[39;00m TermesNormalisation:\n\u001b[0;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m (terme \u001b[39min\u001b[39;00m TermesFrequence\u001b[39m.\u001b[39mkeys()):\n\u001b[0;32m      4\u001b[0m         TermesFrequence[terme] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TermesNormalisation' is not defined"
     ]
    }
   ],
   "source": [
    "TermesFrequence = {}\n",
    "for terme in TermesNormalisation:\n",
    "    if (terme in TermesFrequence.keys()):\n",
    "        TermesFrequence[terme] += 1\n",
    "    else:\n",
    "        TermesFrequence[terme] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d.z.': 1, 'post': 1, 'print': 1, 'cost': 1, '120.50da': 1, '...': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TermesFrequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['d.z.', 'post', 'print', 'cost', '120.50da', '...'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TermesFrequence.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('d.z.', 1), ('post', 1), ('print', 1), ('cost', 1), ('120.50da', 1), ('...', 1)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TermesFrequence.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'d.z.': 1, 'post': 1, 'print': 1, 'cost': 1, '120.50da': 1, '...': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TermesFrequence = nltk.FreqDist(TermesNormalisation)\n",
    "TermesFrequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dict()\n",
    "\n",
    "for i in os.listdir(\"Collection\"):\n",
    "    with open(\"Collection/\"+i,\"r\") as f:\n",
    "        docs[i] = \"\".join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D3.txt': 'In scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed to handle the complex nature of user queries in scientific research. We developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM annotation of the DORIS-MAE dataset resulted in a 500x reduction in cost, without compromising quality. Furthermore, due to the multi-tiered structure of these complex queries, the DORIS-MAE dataset can be extended to over 4,000 sub-query test cases without requiring additional annotation. We evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights the need for better approaches to handle complex, multifaceted queries in scientific research.',\n",
       " 'D1.txt': 'Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs.',\n",
       " 'D4.txt': 'Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, significantly improving the efficiency of LLM-based zero-shot ranking. We test our method using the TREC DL datasets and the BEIR zero-shot document ranking benchmark. The empirical results indicate that our approach considerably reduces computational costs while also retaining high zero-shot ranking effectiveness. ',\n",
       " 'D5.txt': 'Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that captures the position of a query/document in the tree. We demonstrate the effectiveness of EHI on several benchmarks, including de-facto industry standard MS MARCO (Dev set and TREC DL19) datasets. For example, with the same compute budget, EHI outperforms state-of-the-art (SOTA) in by 0.6% (MRR@10) on MS MARCO dev set and by 4.2% (nDCG@10) on TREC DL19 benchmarks. ',\n",
       " 'D2.txt': 'With the , the contextual representation of text data has leveraged the query and the document to be represented in low-dimensional dense vector space. These vectors are learned embeddings of fixed sizes, resulting in deeper text understanding. In this study, we designed a pipeline for effectively retrieving documents from a large search space by combining the deeper text understanding capabilities of the transformer-based BERT model and a phrase embedding-based query expansion model. To learn the contextual representations, we fine-tuned a deep semantic matching model by separately encoding the document and the query. The encoder model is based on the Sentence BERT (SBERT) architecture, which separately generates dense vector representations of documents and queries. The study has also addressed the maximum token length limitation of transformer-based models through the summarization of lengthy documents. In addition, to improve the clarity and completeness of short queries and reduce the semantic gap, a phrase embedding-based query expansion model is employed. The documents and their dense vectors are indexed using the Elasticsearch engine, and matched them with query vectors for retrieving query-specific documents. Finally, the BERT-based cross-encoder model is used to re-rank the relevant records for each query. It performs full self-attention over the inputs, and yields richer text interactions to produce the final results. To assess performance, experiments are conducted on two well-known datasets, TREC-CDS-2014 and OHSUMED. A comparative analysis is carried out, which clearly demonstrates that the proposed framework produced competitive retrieval results.advent of transformer-based architectures',\n",
       " 'D6.txt': 'Query expansion (QE) is commonly used to improve the performance of traditional information retrieval (IR) models. With the adoption of deep learning in IR research, neural QE models have emerged in recent years. Many of these models focus on learning embeddings by leveraging query-document relevance. These embedding models allow computing semantic similarities between queries and documents to generate expansion terms. However, existing models often ignore query-document interactions. This research aims to address that gap by proposing a QE model using a conditional variational autoencoder. It first maps a query-document pair into a latent space based on their interaction, then estimates an expansion model from that latent space. The proposed model is trained on relevance feedback data and generates expansions using pseudo-relevance feedback at test time. The proposed model is evaluated on three standard TREC collections for document ranking: AP and Robust 04 and GOV02, and the MS MARCO dataset for passage ranking. Results show the model outperforms state-of-the-art traditional and neural QE models. It also demonstrates higher additivity with neural matching than baselines.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerSplitPorter(text):\n",
    "    Termes = text.split()\n",
    "    TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "    Porter = nltk.PorterStemmer()\n",
    "    TermesNormalisation = [Porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "    TermesFrequence = nltk.FreqDist(TermesNormalisation)\n",
    "    return TermesFrequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerRegPorter(text):\n",
    "    ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') \n",
    "    Termes = ExpReg.tokenize(text)\n",
    "    TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "    Porter = nltk.PorterStemmer()\n",
    "    TermesNormalisation = [Porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "    TermesFrequence = nltk.FreqDist(TermesNormalisation)\n",
    "    return TermesFrequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerSplitLancaster(text):\n",
    "    Termes = text.split()\n",
    "    TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "    Lancaster = nltk.LancasterStemmer()\n",
    "    TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "    TermesFrequence = nltk.FreqDist(TermesNormalisation)\n",
    "    return TermesFrequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerRegLancaster(text):\n",
    "    ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*')  \n",
    "    Termes = ExpReg.tokenize(text)\n",
    "    TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "    Lancaster = nltk.LancasterStemmer()\n",
    "    TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "    TermesFrequence = nltk.FreqDist(TermesNormalisation)\n",
    "    return TermesFrequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitPorterDict = dict()\n",
    "RegPorterDict = dict()\n",
    "SplitLancasterDict = dict()\n",
    "RegLancasterDict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in docs.keys():\n",
    "    SplitPorterDict[i]= cleanerSplitPorter(docs[i])\n",
    "    RegPorterDict[i]= cleanerRegPorter(docs[i])\n",
    "    SplitLancasterDict[i]= cleanerSplitLancaster(docs[i])\n",
    "    RegLancasterDict[i]= cleanerRegLancaster(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D3.txt': FreqDist({'queri': 5, 'dataset': 5, 'complex': 5, 'scientif': 4, 'annot': 4, 'retriev': 3, 'relev': 3, 'document': 3, 'effect': 2, 'complex,': 2, ...}),\n",
       " 'D1.txt': FreqDist({'rerank': 5, 'research': 2, 'model': 2, 'work': 2, 'behind': 2, 'experiment': 2, 'result': 2, 'zero-shot': 2, 'effect': 2, 'success': 1, ...}),\n",
       " 'D4.txt': FreqDist({'approach': 7, 'zero-shot': 6, 'rank': 4, 'effect': 3, 'prompt': 3, 'llm-base': 3, 'effici': 3, 'high': 3, 'model': 2, 'demonstr': 2, ...}),\n",
       " 'D5.txt': FreqDist({'learn': 5, 'ehi': 5, 'document': 4, 'embed': 4, 'standard': 3, 'ann': 3, 'dens': 2, 'retriev': 2, 'industri': 2, 'search': 2, ...}),\n",
       " 'D2.txt': FreqDist({'model': 6, 'queri': 5, 'document': 5, 'vector': 5, 'text': 4, 'dens': 3, 'retriev': 3, 'transformer-bas': 3, 'contextu': 2, 'represent': 2, ...}),\n",
       " 'D6.txt': FreqDist({'model': 9, 'expans': 4, 'use': 3, 'neural': 3, 'qe': 3, 'query-docu': 3, 'propos': 3, 'queri': 2, 'tradit': 2, 'models.': 2, ...})}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SplitPorterDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D3.txt': 'In scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed to handle the complex nature of user queries in scientific research. We developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM annotation of the DORIS-MAE dataset resulted in a 500x reduction in cost, without compromising quality. Furthermore, due to the multi-tiered structure of these complex queries, the DORIS-MAE dataset can be extended to over 4,000 sub-query test cases without requiring additional annotation. We evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights the need for better approaches to handle complex, multifaceted queries in scientific research.',\n",
       " 'D1.txt': 'Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs.',\n",
       " 'D4.txt': 'Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, significantly improving the efficiency of LLM-based zero-shot ranking. We test our method using the TREC DL datasets and the BEIR zero-shot document ranking benchmark. The empirical results indicate that our approach considerably reduces computational costs while also retaining high zero-shot ranking effectiveness. ',\n",
       " 'D5.txt': 'Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that captures the position of a query/document in the tree. We demonstrate the effectiveness of EHI on several benchmarks, including de-facto industry standard MS MARCO (Dev set and TREC DL19) datasets. For example, with the same compute budget, EHI outperforms state-of-the-art (SOTA) in by 0.6% (MRR@10) on MS MARCO dev set and by 4.2% (nDCG@10) on TREC DL19 benchmarks. ',\n",
       " 'D2.txt': 'With the , the contextual representation of text data has leveraged the query and the document to be represented in low-dimensional dense vector space. These vectors are learned embeddings of fixed sizes, resulting in deeper text understanding. In this study, we designed a pipeline for effectively retrieving documents from a large search space by combining the deeper text understanding capabilities of the transformer-based BERT model and a phrase embedding-based query expansion model. To learn the contextual representations, we fine-tuned a deep semantic matching model by separately encoding the document and the query. The encoder model is based on the Sentence BERT (SBERT) architecture, which separately generates dense vector representations of documents and queries. The study has also addressed the maximum token length limitation of transformer-based models through the summarization of lengthy documents. In addition, to improve the clarity and completeness of short queries and reduce the semantic gap, a phrase embedding-based query expansion model is employed. The documents and their dense vectors are indexed using the Elasticsearch engine, and matched them with query vectors for retrieving query-specific documents. Finally, the BERT-based cross-encoder model is used to re-rank the relevant records for each query. It performs full self-attention over the inputs, and yields richer text interactions to produce the final results. To assess performance, experiments are conducted on two well-known datasets, TREC-CDS-2014 and OHSUMED. A comparative analysis is carried out, which clearly demonstrates that the proposed framework produced competitive retrieval results.advent of transformer-based architectures',\n",
       " 'D6.txt': 'Query expansion (QE) is commonly used to improve the performance of traditional information retrieval (IR) models. With the adoption of deep learning in IR research, neural QE models have emerged in recent years. Many of these models focus on learning embeddings by leveraging query-document relevance. These embedding models allow computing semantic similarities between queries and documents to generate expansion terms. However, existing models often ignore query-document interactions. This research aims to address that gap by proposing a QE model using a conditional variational autoencoder. It first maps a query-document pair into a latent space based on their interaction, then estimates an expansion model from that latent space. The proposed model is trained on relevance feedback data and generates expansions using pseudo-relevance feedback at test time. The proposed model is evaluated on three standard TREC collections for document ranking: AP and Robust 04 and GOV02, and the MS MARCO dataset for passage ranking. Results show the model outperforms state-of-the-art traditional and neural QE models. It also demonstrates higher additivity with neural matching than baselines.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(data,wordcounter):\n",
    "    tfIDF = dict()\n",
    "    for doc_name,FreqDist in data.items():\n",
    "        tfIDF[doc_name] = dict()\n",
    "        for i,j in dict(FreqDist).items():\n",
    "            tfIDF[doc_name][i] =(j/ max(dict(FreqDist).values()))*math.log10(len(data)/wordcounter[i]+1)\n",
    "    return tfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcounter(docs):\n",
    "    dictOfWord = dict()\n",
    "    for _,FreqDist in docs.items():\n",
    "        for i in dict(FreqDist).keys():\n",
    "            if not dictOfWord.get(i):\n",
    "                dictOfWord[i] = 0\n",
    "                for _,d in docs.items():\n",
    "                    if d.get(i):\n",
    "                        dictOfWord[i] +=1\n",
    "    return dictOfWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterSP = wordcounter(SplitPorterDict)\n",
    "counterRP = wordcounter(RegPorterDict)\n",
    "counterSL = wordcounter(SplitLancasterDict)\n",
    "counterRL = wordcounter(RegLancasterDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfSP = TFIDF(SplitPorterDict,counterSP)\n",
    "tfidfRP = TFIDF(RegPorterDict,counterRP)\n",
    "tfidfSL = TFIDF(SplitLancasterDict,counterSL)\n",
    "tfidfRL = TFIDF(RegLancasterDict,counterRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDesc(docs,tfidf,fname):\n",
    "    with open(fname,\"w\") as f:\n",
    "        for name,FreqDist in docs.items():\n",
    "            ndoc =name[1]\n",
    "            for i in dict(FreqDist).keys():\n",
    "                f.write(f\"{ndoc} {i} {FreqDist[i]} {tfidf[name][i]}\\n\")\n",
    "def saveinvers(docs,tfidf,fname):\n",
    "    with open(fname,\"w\") as f:\n",
    "        for name,FreqDist in docs.items():\n",
    "            ndoc =name[1]\n",
    "            for i in dict(FreqDist).keys():\n",
    "                f.write(f\"{i} {ndoc} {FreqDist[i]} {tfidf[name][i]}\\n\")         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saveDesc(SplitPorterDict,tfidfSP,\"output/DescripteursSplitPorter.txt\")\n",
    "saveDesc(RegPorterDict,tfidfRP,\"output/DescripteursTokenPorter.txt\")\n",
    "saveDesc(SplitLancasterDict,tfidfSL,\"output/DescripteursSplitLancaster.txt\")\n",
    "saveDesc(RegLancasterDict,tfidfRL,\"output/DescripteursTokenLancaster.txt\")\n",
    "saveinvers(SplitPorterDict,tfidfSP,\"output/InverseSplitPorter.txt\")\n",
    "saveinvers(RegPorterDict,tfidfRP,\"output/InverseTokenPorter.txt\")\n",
    "saveinvers(SplitLancasterDict,tfidfSL,\"output/InverseSplitLancaster.txt\")\n",
    "saveinvers(RegLancasterDict,tfidfRL,\"output/InverseTokenLancaster.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
